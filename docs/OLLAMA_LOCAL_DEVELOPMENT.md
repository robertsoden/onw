# Ollama - Local LLM Development Guide

**Date:** November 16, 2025
**Version:** 1.0
**Purpose:** Run Ontario Nature Watch with local LLMs for development

---

## Overview

**Ollama** allows you to run large language models locally on your machine without requiring API keys or internet access. This is ideal for:

- ðŸ”’ **Privacy** - All data stays on your machine
- ðŸ’° **Cost** - No API costs during development
- âš¡ **Speed** - No network latency
- ðŸ› ï¸ **Development** - Test without rate limits

---

## Quick Start

### 1. Install Ollama

**macOS / Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download from https://ollama.com/download/windows

**Verify Installation:**
```bash
ollama --version
```

### 2. Pull a Model

**Recommended for Development:**
```bash
# Small & Fast (3B parameters, ~2GB)
ollama pull llama3.2:3b

# Balanced (14B parameters, ~8GB)
ollama pull qwen2.5:14b

# Best Quality (70B parameters, ~40GB)
ollama pull llama3.3:70b
```

### 3. Install Python Package

```bash
# In your virtual environment
pip install langchain-ollama
```

### 4. Configure Environment

```bash
# In .env file
MODEL=llama3.2          # Use Ollama model
SMALL_MODEL=llama3.2    # Use same for small model

# Optional: Custom Ollama URL (default is http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434
```

### 5. Start Ontario Nature Watch

```bash
# Ollama service starts automatically when you pull a model
# Just run your app normally
python -m src.main
```

---

## Available Models

### Recommended Models

| Model | Size | RAM Needed | Speed | Quality | Best For |
|-------|------|------------|-------|---------|----------|
| **llama3.2:3b** | 2GB | 8GB | âš¡âš¡âš¡ Fast | â­â­â­ Good | Quick testing |
| **qwen2.5:14b** | 8GB | 16GB | âš¡âš¡ Fast | â­â­â­â­ Excellent | Coding tasks |
| **phi4:14b** | 8GB | 16GB | âš¡âš¡ Fast | â­â­â­â­ Excellent | General purpose |
| **llama3.3:70b** | 40GB | 64GB+ | âš¡ Slow | â­â­â­â­â­ Best | Production quality |
| **mistral-large** | 120GB | 128GB+ | âš¡ Slow | â­â­â­â­â­ Best | Research |
| **deepseek-r1:7b** | 4GB | 8GB | âš¡âš¡âš¡ Fast | â­â­â­â­ Excellent | Reasoning tasks |

### Configuration Names

Use these names in `.env`:

```bash
# Llama models
MODEL=llama3.3       # 70B - Best quality
MODEL=llama3.2       # 3B - Fast

# Qwen models (excellent for coding)
MODEL=qwen2.5        # 32B - Balanced
MODEL=qwen2.5-14b    # 14B - Faster

# Other models
MODEL=phi4           # Microsoft's compact model
MODEL=mistral        # Mistral Large
MODEL=deepseek       # DeepSeek R1
```

---

## Hardware Requirements

### Minimum (for small models)

- **CPU:** Modern multi-core (M1/M2 Mac, Intel i5+, AMD Ryzen 5+)
- **RAM:** 8GB
- **Storage:** 10GB free
- **Model:** llama3.2:3b

### Recommended (for development)

- **CPU:** Apple Silicon M1/M2/M3, Intel i7/i9, AMD Ryzen 7/9
- **RAM:** 16GB+
- **GPU:** Optional (Apple Metal, NVIDIA CUDA)
- **Storage:** 50GB free
- **Model:** qwen2.5:14b or phi4:14b

### High-End (for production-quality)

- **CPU:** M2 Ultra, Intel i9, AMD Threadripper
- **RAM:** 64GB+
- **GPU:** NVIDIA RTX 4090 (24GB VRAM) or Apple M2 Ultra
- **Storage:** 100GB+ free
- **Model:** llama3.3:70b or mistral-large

---

## Usage Examples

### Example 1: Quick Testing

```bash
# Use fast model for quick iteration
MODEL=llama3.2 python -m src.main

# Query Ontario statistics
curl -X POST http://localhost:8000/api/ontario/statistics \
  -H "Content-Type: application/json" \
  -d '{"area_name": "Algonquin Park", "metric": "biodiversity"}'
```

### Example 2: Quality Development

```bash
# Use high-quality model for realistic testing
MODEL=qwen2.5 SMALL_MODEL=llama3.2 python -m src.main
```

### Example 3: GPU Acceleration

```bash
# Ollama automatically uses GPU if available
# Check GPU usage:
nvidia-smi  # NVIDIA
# or
Activity Monitor â†’ GPU History  # macOS
```

---

## Performance Comparison

### Ontario Statistics Query

| Model | First Response | Subsequent | Quality |
|-------|----------------|------------|---------|
| **Cloud (Gemini)** | 2-5s | 2-5s | â­â­â­â­â­ |
| **llama3.2:3b** | 1-3s | 0.5-2s | â­â­â­ |
| **qwen2.5:14b** | 3-8s | 2-5s | â­â­â­â­ |
| **llama3.3:70b** | 10-30s | 5-15s | â­â­â­â­â­ |

*Times on M2 MacBook Pro (16GB RAM)*

### Memory Usage

```bash
# Check Ollama memory usage
ps aux | grep ollama

# Monitor in real-time
watch -n 1 'ps aux | grep ollama'
```

---

## Troubleshooting

### Problem 1: "Ollama not found"

**Solution:**
```bash
# Check if Ollama is running
ps aux | grep ollama

# Start Ollama service
ollama serve

# Or restart
sudo systemctl restart ollama  # Linux
# On macOS, Ollama starts automatically
```

### Problem 2: Model Not Found

**Error:** `Model 'llama3.2:3b' not found`

**Solution:**
```bash
# List available models
ollama list

# Pull the model
ollama pull llama3.2:3b

# Verify
ollama run llama3.2:3b "Hello"
```

### Problem 3: Out of Memory

**Error:** System freezes or crashes

**Solution:**
```bash
# Use smaller model
MODEL=llama3.2 python -m src.main

# Or limit concurrent requests in Ollama
export OLLAMA_NUM_PARALLEL=1
ollama serve
```

### Problem 4: Slow Performance

**Solutions:**

1. **Use GPU acceleration:**
   ```bash
   # Check if GPU is being used
   ollama ps  # Shows running models and GPU usage
   ```

2. **Use smaller model:**
   ```bash
   MODEL=llama3.2 python -m src.main
   ```

3. **Keep model loaded:**
   ```bash
   # Preload model to avoid cold starts
   ollama run llama3.2:3b ""
   ```

4. **Increase context:**
   ```bash
   # If model keeps reloading
   export OLLAMA_KEEP_ALIVE=30m
   ollama serve
   ```

### Problem 5: Connection Refused

**Error:** `Connection refused to http://localhost:11434`

**Solution:**
```bash
# Check if Ollama is running
curl http://localhost:11434

# Start Ollama
ollama serve

# Or check custom URL
echo $OLLAMA_BASE_URL
```

---

## Best Practices

### Development Workflow

```bash
# 1. Start with fast model for iteration
MODEL=llama3.2 python -m src.main

# 2. Test with quality model before commit
MODEL=qwen2.5 pytest tests/

# 3. Final check with cloud model
MODEL=gemini pytest tests/
```

### Model Selection Guide

**Quick Iteration:**
- Use: `llama3.2:3b`
- When: Rapid testing, debugging
- Trade-off: Lower quality responses

**Development:**
- Use: `qwen2.5:14b` or `phi4:14b`
- When: Building features, testing logic
- Trade-off: Slower, but good quality

**Pre-Production:**
- Use: `llama3.3:70b` or cloud models
- When: Final testing, quality assurance
- Trade-off: Slow, high memory, best quality

### Cost Savings

**Estimated API cost savings:**

```
Development phase (100 requests/day Ã— 30 days):
- Cloud (Gemini): ~$50-100/month
- Ollama: $0

Testing phase (1000 requests/day):
- Cloud (Gemini): ~$500-1000/month
- Ollama: $0

One-time cost:
- Hardware: Already have
- Electricity: ~$5/month (24/7 usage)
```

---

## Integration with Ontario Nature Watch

### Automatic Model Selection

The system automatically detects Ollama availability:

```python
# From src/utils/llms.py
if OLLAMA_AVAILABLE:
    MODEL_REGISTRY.update({
        "llama3.3": LLAMA_3_3_70B,
        "qwen2.5": QWEN_2_5_32B,
        # ... other models
    })
```

### Testing Ontario Data Handler

```bash
# Use Ollama for testing
MODEL=llama3.2 pytest tests/tools/test_ontario_handler.py -v

# Compare with cloud model
MODEL=gemini pytest tests/tools/test_ontario_handler.py -v
```

### Ontario Statistics with Ollama

```python
# The tool works identically with Ollama
from src.tools.ontario.get_ontario_statistics import get_ontario_statistics

# Ollama will process this just like cloud models
result = await get_ontario_statistics(
    area_name="Algonquin Park",
    metric="biodiversity"
)
```

---

## Advanced Configuration

### Multiple Ollama Instances

```bash
# Run multiple Ollama services
OLLAMA_BASE_URL=http://localhost:11434 ollama serve &
OLLAMA_BASE_URL=http://localhost:11435 ollama serve &

# Configure app
export OLLAMA_BASE_URL=http://localhost:11435
```

### Custom Models

```bash
# Create custom Modelfile
cat > Modelfile << 'EOF'
FROM llama3.2:3b
PARAMETER temperature 0
PARAMETER top_k 40
PARAMETER top_p 0.9
EOF

# Build custom model
ollama create ontario-dev -f Modelfile

# Use it
MODEL=ontario-dev python -m src.main
```

### GPU Configuration

**NVIDIA:**
```bash
# Check GPU support
nvidia-smi

# Ollama uses GPU automatically
# No configuration needed
```

**Apple Silicon:**
```bash
# Ollama automatically uses Metal
# Check usage in Activity Monitor â†’ GPU History
```

---

## Model Management

### Listing Models

```bash
# Show installed models
ollama list

# Show running models
ollama ps

# Show model info
ollama show llama3.2:3b
```

### Removing Models

```bash
# Free up space
ollama rm llama3.2:3b

# Remove all except one
ollama list | grep -v "llama3.2" | awk '{print $1}' | xargs -I {} ollama rm {}
```

### Updating Models

```bash
# Update to latest version
ollama pull llama3.2:3b

# Pulls update if available
```

---

## FAQ

### Q: Do I need internet?

**A:** Only to download models initially. After that, everything runs offline.

### Q: Can I use Ollama in production?

**A:** Ollama is best for development. For production, use cloud APIs (Gemini, GPT) for reliability and scalability.

### Q: Which model is closest to Gemini?

**A:** `llama3.3:70b` or `mistral-large` provide similar quality, but are much slower.

### Q: Can I mix Ollama and cloud models?

**A:** Yes! Use Ollama for `MODEL` and cloud for `SMALL_MODEL`:
```bash
MODEL=llama3.2 SMALL_MODEL=gemini-flash python -m src.main
```

### Q: How much does it cost?

**A:** Ollama is free. You only pay for:
- Hardware (you likely already have)
- Electricity (~$5/month if running 24/7)

### Q: Is it secure?

**A:** Yes! All data stays on your machine. No data sent to external APIs.

---

## Resources

**Official Sites:**
- Ollama: https://ollama.com
- Model Library: https://ollama.com/library
- GitHub: https://github.com/ollama/ollama

**Model Documentation:**
- Llama 3.2: https://ollama.com/library/llama3.2
- Qwen 2.5: https://ollama.com/library/qwen2.5
- Phi-4: https://ollama.com/library/phi4

**Community:**
- Ollama Discord: https://discord.gg/ollama
- Reddit: r/LocalLLaMA

---

## Summary

**Setup:**
1. Install Ollama: `curl -fsSL https://ollama.com/install.sh | sh`
2. Pull model: `ollama pull llama3.2:3b`
3. Install package: `pip install langchain-ollama`
4. Configure: `MODEL=llama3.2` in `.env`
5. Run: `python -m src.main`

**Recommended Models:**
- **Fast:** llama3.2:3b (2GB)
- **Balanced:** qwen2.5:14b (8GB)
- **Quality:** llama3.3:70b (40GB)

**Benefits:**
- âœ… No API costs
- âœ… Offline development
- âœ… No rate limits
- âœ… Privacy

**Trade-offs:**
- âŒ Slower than cloud
- âŒ Requires hardware
- âŒ Lower quality (small models)

---

**Document Version:** 1.0
**Last Updated:** November 16, 2025
**Status:** Production Ready
